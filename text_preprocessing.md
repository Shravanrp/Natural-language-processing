
---

# ğŸ§  Advanced Text Preprocessor

A **complete, production-ready text preprocessing pipeline** for Natural Language Processing (NLP) tasks â€” implemented in pure Python using best industry practices.

This module provides **cleaning**, **normalization**, **tokenization**, **linguistic processing**, and **text augmentation** in a single, modular class: `AdvancedTextPreprocessor`.

---

## ğŸš€ Features

âœ… **Raw Text Cleaning**

* Removes HTML, URLs, emails, and unwanted characters
* Expands contractions (e.g., *canâ€™t â†’ cannot*)
* Handles special symbols and numbers

âœ… **Normalization**

* Converts to lowercase
* Removes punctuation
* Normalizes whitespace
* Replaces slang and abbreviations (*e.g., omg â†’ oh my god*)

âœ… **Tokenization**

* Supports multiple methods: `whitespace`, `word`, and `sentence`
* Produces clean, consistent tokens for modeling

âœ… **Linguistic Processing**

* Removes stopwords
* Performs simple POS tagging
* Lemmatizes or stems words
* Extracts Named Entities (NER) like names, emails, dates, and phone numbers

âœ… **Text Enhancement**

* Spell correction (basic dictionary-based)
* Synonym replacement for data augmentation
* Configurable full preprocessing pipeline

---

## ğŸ—ï¸ Project Structure

```
advanced_text_preprocessor/
â”‚
â”œâ”€â”€ ğŸ“„ advanced_text_preprocessor.py     # Core module (the class you provided)
â”œâ”€â”€ ğŸ“˜ README.md                         # This documentation              
```

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/Shravanrp/advanced-text-preprocessor.git
cd advanced-text-preprocessor
```

---

## ğŸ§© Usage

### â–¶ï¸ Basic Example

```python
from advanced_text_preprocessor import AdvancedTextPreprocessor

preprocessor = AdvancedTextPreprocessor()

text = "OMG! I can't believe this happened 2day ğŸ˜±. Email me at test@example.com."

results = preprocessor.preprocess_pipeline(
    text,
    expand_contractions=True,
    normalize_slang=True,
    lemmatize=True,
    correct_spelling=True,
    extract_ner=True
)

print("Final Tokens:", results['final_tokens'])
print("Clean Text:", results['final_text'])
print("Named Entities:", results.get('step12_entities', {}))
```

---

### ğŸ§¾ Example Output

```
Original: OMG! I can't believe this happened 2day ğŸ˜±. Email me at test@example.com.

Step 1 (Cleaned): OMG! I can't believe this happened 2day . Email me at 
Step 2 (Contractions): OMG! I cannot believe this happened 2day . Email me at 
Step 4 (Lowercase): omg! i cannot believe this happened 2day . email me at 
Step 7 (Slang normalized): oh my god i cannot believe this happened today email me at
Step 8 (Tokens): ['oh', 'my', 'god', 'i', 'cannot', 'believe', 'this', 'happened', 'today', 'email', 'me', 'at']
Step 9 (Stopwords removed): ['god', 'cannot', 'believe', 'happened', 'today', 'email']
Step 11 (Lemmatized): ['god', 'cannot', 'believe', 'happen', 'today', 'email']
Step 13 (Spelling corrected): ['god', 'cannot', 'believe', 'happen', 'today', 'email']

âœ… Final Tokens: ['god', 'cannot', 'believe', 'happen', 'today', 'email']
âœ… Final Text: "god cannot believe happen today email"
```

---

## âš™ï¸ Customization

You can toggle features in the pipeline:

```python
results = preprocessor.preprocess_pipeline(
    text,
    expand_contractions=False,
    remove_numbers=True,
    normalize_slang=False,
    pos_tagging=True,
    stem=True,
    replace_synonyms=True
)
```

You can also call individual steps manually:

```python
clean_text = preprocessor.remove_unwanted_chars(text)
tokens = preprocessor.tokenize(clean_text, method='word')
filtered = preprocessor.remove_stopwords(tokens)
lemmas = preprocessor.lemmatize_tokens(filtered)
```

---

## ğŸ§  Pipeline Order (Summary)

| Step | Task                     | Function                               | Example                         |
| ---- | ------------------------ | -------------------------------------- | ------------------------------- |
| 1    | Remove unwanted chars    | `remove_unwanted_chars()`              | remove HTML, URLs               |
| 2    | Expand contractions      | `expand_contractions()`                | canâ€™t â†’ cannot                  |
| 3    | Remove symbols/numbers   | `remove_special_symbols()`             | `@#12` â†’                        |
| 4    | Lowercase                | `lowercase()`                          | NLP â†’ nlp                       |
| 5    | Remove punctuation       | `remove_punctuation()`                 | wow! â†’ wow                      |
| 6    | Normalize whitespace     | `normalize_whitespace()`               | â€œhello   worldâ€ â†’ â€œhello worldâ€ |
| 7    | Normalize slang          | `normalize_slang()`                    | omg â†’ oh my god                 |
| 8    | Tokenize                 | `tokenize()`                           | split text into words           |
| 9    | Remove stopwords         | `remove_stopwords()`                   | â€œtheâ€, â€œisâ€, â€œandâ€ removed      |
| 10   | POS tagging              | `pos_tag_simple()`                     | simple grammar-based tags       |
| 11   | Lemmatize / Stem         | `lemmatize_tokens()` / `stem_tokens()` | â€œrunningâ€ â†’ â€œrunâ€               |
| 12   | Named Entity Recognition | `extract_entities()`                   | detect emails, names, etc.      |
| 13   | Spell correction         | `correct_spelling()`                   | â€œtehâ€ â†’ â€œtheâ€                   |
| 14   | Synonym replacement      | `replace_with_synonyms()`              | â€œgoodâ€ â†’ â€œgreatâ€                |
                                 |

---


---

## ğŸ§© Future Improvements

* Integrate spaCy / NLTK POS tagging and lemmatization
* Add configurable stemming rules (Porter / Snowball)
* Extend synonym replacement via WordNet
* Add multilingual support

---

## ğŸ§‘â€ğŸ’» Author

**Shravan Pattar**
ğŸ“˜ NLP Developer | Machine Learning Enthusiast

---
